---
title: "Scallop dredge comparison draft RMD"
author: "Alex Reich"
date: "2023-10-09"
output: html_document
---


# Dredge comparison study


## Take-away message/Abstract
Apply an FPC for large scallops, but not for small scallops. The size distribution of catch between the Homer and Kodiak dredges are similar, according to the Kolmogorov-Smirnov test. I've identified 2 outliers for the large scallops and 4 outliers for the small scallops. Using the CPUE based on weight for analysis, the FPC for a Kodiak to Homer dredge conversion was 0.68 for large scallops and 0.74 for small scallops. Only the large scallops had a confidence interval that did not overlap with one. The plot of predicted values over observed values for the ANOVA looks bad for the higher CPUE hauls.

The results of the MSE simulation indicate that the MSE is reduced when both the large and small scallop FPC's are applied.

Since the large scallop FPC has CI's that do not overlap with one and reduces MSE, my analysis indicates that we should apply the FPC to the large scallops dataset. The small scallop FPC had CI's that did overlap with 1.

Normality: does not look terrible terrible when graphed but also does not look great. According to the Shapiro-Wilk test, both large and small scallop datasets are non-normal. When outliers are removed, the large scallop dataset is non-normal and the small scallop dataset is normal, according to the Shapiro-Wilk test. Since the randomized block ANOVA assumes normality, this could be an indication to pick a different analysis method.

I also tried the Kappenman's method. It had more conservative results. FPC was 0.84 for large scallops and 0.73 for small scallops (using the weight-based CPUE for calculation). Both CI's for the large and small scallop Kappenman FPC's overlap with one, indicating FPC's should not be accepted. This test does not assume normality, which is a plus. However, CPUE's of 0 are deleted and paired hauls are not accounted for, which is undesirable here. How important are the paired hauls where one net got 0? Is that a fluke? Should those hauls be removed from analysis? They are all outliers...

In future iterations of analysis, I may want to eliminate the outliers? The outliers occur when one dredge catches 0 scallops and the other one catches not zero scallops. This could be something to ask the scallop team about.

Results after outliers are removed:
- ANOVA: a higher precision. See table. After removing the outliers, both small and large scallops have confidence intervals outside of 1.




## Analysis/code

Load packages
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(RColorBrewer)
library(cowplot)
library(fishmethods)
library(viridis)

set.seed(123)
```



Read in data. 
```{r}
## logbook data
logbook <- read.csv("./data_folder/new_data_dump/DredgeSurvey_FishLogData_CatchComparisonHauls.csv") 

## catch data
catch_raw <- read.csv("./data_folder/new_data_dump/DredgeSurvey_CatchData_CatchComparisonHauls.csv") 

## specimen data
specimen <- read.csv("./data_folder/new_data_dump/DredgeSurvey_ScallopBioData_CatchComparisonHauls.csv")
```


Data wrangle.
```{r}
# join logbook and catch in a way that preserves zero catches
tidyr::expand_grid(logbook,
            dplyr::distinct(dplyr::transmute(catch_raw, samp_grp, rcode, comname, whole_haul, sample_type))) %>% #expand logbook to all catch types
  dplyr::left_join(catch_raw) %>% #join with the catch.This connects catch vales to gear_code id.
  # fill in zero catches
  tidyr::replace_na(list(samp_cnt = 0, samp_wt = 0)) %>%
  # filter for scallops
  dplyr::filter(rcode == 74120) -> catch # now you have records of scallop catch per size class (samp_grp) per gear type
  

# cpue will be samp_wt (or count) / area_swept_sqnm. This code is calculating CPUE for both weight and number of scallops
catch_cpue <- catch %>% 
    dplyr::mutate(cpue_cnt = samp_cnt/area_swept_sqnm,
           cpue_wt = samp_wt/area_swept_sqnm)

# compute sample factor for specimen data 
specimen %>%
  # group by tow, samp grp, and size to get n at size
  dplyr::group_by(tow, samp_grp, shell_height) %>%
  dplyr::summarize(count = n()) %>%
  # compute number measured per tow
  dplyr::group_by(tow, samp_grp) %>%
  dplyr::mutate(n_measured = n()) %>% dplyr::ungroup() %>%
  # join to catch data
  dplyr::left_join(catch, by = c("tow", "samp_grp")) %>%
  # compute sample factor
  dplyr::mutate(sample_factor = samp_cnt * (count / n_measured)) -> specimen

#separate large and small scallops for cpue
large <- catch_cpue %>% dplyr::filter(samp_grp == 1)
small <- catch_cpue %>% dplyr::filter(samp_grp == 2)

##Remove where both hauls are 0.
large_2 <- large %>% #no change
    dplyr::group_by(haul) %>%
    dplyr::filter(!all(cpue_cnt == 0)) %>%
    dplyr::ungroup()

small_2 <- small %>% #filtered out where both dredges in a haul caught nothing
    dplyr::group_by(haul) %>%
    dplyr::filter(!all(cpue_cnt == 0)) %>%
    dplyr::ungroup()

#code the factors as factors
large_m <- large_2 %>%
           dplyr::mutate(gear_code = base::factor(gear_code, levels=c("1","5")), haul = base::factor(haul))  #the levels argument specifies that Kodiak(5) is the base, to compare Homer(1) against
    

small_m <- small_2 %>%
           dplyr::mutate(gear_code = base::factor(gear_code, levels=c("1", "5")), haul = base::factor(haul)) #levels=c(1,5) ?

```


Exploratory graphs
```{r}
##separate into large and small scallops

hist(log(large$cpue_wt+1), breaks=20) #try smaller bins?
hist(large$cpue_wt, breaks=20)
hist(small$cpue_wt, breaks=20)
hist(log(small$cpue_wt), breaks=20)
hist(log(large$cpue_wt), breaks=20)
hist(log(small$cpue_cnt), breaks=20)
hist(log(large$cpue_cnt), breaks=20)



#gamma explore
hist(large$cpue_wt, breaks=20)
hist(small$cpue_wt, breaks=20)
x<-rgamma(n=1000, shape=2, rate=1)
hist(x)
x<-rgamma(n=1000, shape=0.7, rate=1)
hist(x, breaks=20)
```




## Data analysis

### ANOVA method
```{r}
options(contrasts = rep("contr.sum", 2)) #to use this or not?????
#options(contrasts = rep("contr.treatment", 2)) #oh. This way does not give me the grand mean as the intercept.
#log(cpue+1) ~ grand mean + net effect (KENAI OR HOMER) + haul effect (HAUL #)
##large scallops
#large_m$gear_code <- relevel(large_m$gear_code, ref="5") #triedd this, decided no

mod1_large_wt <- stats::lm(log(cpue_wt+1) ~ gear_code + haul, data=large_m) #things are different after this runs through. What has happened?
#haul is somewhere? the nammes are named somethign different?
#something about running fishmethods::fpc with method=2 (randomized block anova) messes with the lm function.

mod1_large_cnt <- lm(log(cpue_cnt+1) ~ gear_code + haul, data=large_m)
#mod1_large_cnt_TEST <- aov(log(cpue_cnt+1) ~ grand_mean_cpue_wt + gear_code + haul, data=large_m)
##same same, different way

##small scallops
mod1_small_wt <- lm(log(cpue_wt+1) ~ gear_code + haul, data=small_m)
mod1_small_cnt <- lm(log(cpue_cnt+1) ~ gear_code + haul, data=small_m)

#summary
sum_large_wt <- summary(mod1_large_wt)
sum_large_cnt <- summary(mod1_large_cnt)
sum_small_wt <- summary(mod1_small_wt)
sum_small_cnt <- summary(mod1_small_cnt)

```

Get FPC table from ANOVA
```{r}
##calculate the Fishing Power Correction (FPC) estimator
s_large_wt <- sum_large_wt$coefficients[2,2]  #SE for large scallops
g_large_wt <- sum_large_wt$coefficients[2,1] #gear difference parameter for large scallops

FPC_large_wt <- exp(2*g_large_wt*(1+(0.5*(s_large_wt^2)))) 


s_large_cnt <- sum_large_cnt$coefficients[2,2]  #SE for large scallops
g_large_cnt <- sum_large_cnt$coefficients[2,1] #gear difference parameter for large scallops

FPC_large_cnt <- exp(2*g_large_cnt*(1+0.5*(s_large_cnt^2)))


s_small_wt <- sum_small_wt$coefficients[2,2]  
g_small_wt <- sum_small_wt$coefficients[2,1] 

FPC_small_wt <- exp(2*g_small_wt*(1+0.5*(s_small_wt^2)))


s_small_cnt <- sum_small_cnt$coefficients[2,2]  
g_small_cnt <- sum_small_cnt$coefficients[2,1] 

FPC_small_cnt <- exp(2*g_small_cnt*(1+0.5*(s_small_cnt^2)))

FPC <- c(FPC_large_wt, FPC_large_cnt, FPC_small_wt, FPC_small_cnt)
Category <- c("large (weight)" , "large(count)", "small(weight)", "small(count)")

scallop_FPC_table <- data.frame(Category, FPC)

#CALCULATE CONFIDENCE INTERVALS
##95%CI = exp(2g (+-) 1.96 * 2sqrt(SE^2))

upper_CI_large_wt <- exp(2*g_large_wt + 1.96*2*s_large_wt)
lower_CI_large_wt <- exp(2*g_large_wt - 1.96*2*s_large_wt)
upper_CI_large_cnt<- exp(2*g_large_cnt + 1.96*2*s_large_cnt)
lower_CI_large_cnt<- exp(2*g_large_cnt - 1.96*2*s_large_cnt)
upper_CI_small_wt <- exp(2*g_small_wt + 1.96*2*s_small_wt)
lower_CI_small_wt <- exp(2*g_small_wt - 1.96*2*s_small_wt)
upper_CI_small_cnt <-exp(2*g_small_cnt + 1.96*2*s_small_cnt)
lower_CI_small_cnt <-exp(2*g_small_cnt - 1.96*2*s_small_cnt)


upper_CI <- c(upper_CI_large_wt, upper_CI_large_cnt, upper_CI_small_wt, upper_CI_small_cnt)
lower_CI <- c(lower_CI_large_wt, lower_CI_large_cnt, lower_CI_small_wt,lower_CI_small_cnt)


scallop_FPC_table$Upper_CI <- upper_CI
scallop_FPC_table$Lower_CI <- lower_CI

scallop_FPC_table #apply FPC for large scallops, but not small scallops. Does MSE support this?

##
#10/04/23 troubleshooting
#calc FPC the other way?
#no, the issue was with lm, right??
#make sure factors, and factor orders, are set. Try changing the order of the facotrs in the lm?

```
	
	
	
	

Says for large apply the FPC, small do not apply the FPC. But I should look at the MSE results as well.


Graph fitted values 
```{r}

#large scallops fitted values
fitted_values_test <- predict(mod1_large_wt, type="response")
large_m$fitted_cpue_wt <- exp(fitted_values_test)-1

ggplot(large_m) + aes(x=haul, shape=gear_code, group= gear_code, size =3) +
    geom_point(aes(y=cpue_wt), color="black", alpha=0.5)+
    geom_point(aes(y=fitted_cpue_wt), color="red", alpha=0.3)


#small scallops fitted values

fitted_values_test <- predict(mod1_small_wt, type="response")
small_m$fitted_cpue_wt <- exp(fitted_values_test)-1

ggplot(small_m) + aes(x=haul, shape=gear_code, group= gear_code, size=3) +
    geom_point(aes(y=cpue_wt), color="black", alpha=0.5)+
    geom_point(aes(y=fitted_cpue_wt), color="red", alpha=0.3)

```




Aside: ratio of one to the other 
```{r}
large_kodiak <- large_m %>% filter(gear_code==5)
large_homer <- large_m %>% filter(gear_code==1)

large_kodiak_mean <- mean(large_kodiak$cpue_wt)
large_homer_mean <- mean(large_homer$cpue_wt)

large_homer_mean/large_kodiak_mean

#hmm
```


More plots
```{r}
ggplot(large_m) + aes(x=bed_name, y=log(cpue_wt+1)) + geom_boxplot()
ggplot(large_m) + aes(x=factor(haul), y=log(cpue_wt+1)) + geom_boxplot() #higher hauls have higher cpue
ggplot(large_m) + aes(x=factor(haul), y=log(cpue_wt+1), color=bed_name) + geom_boxplot()
ggplot(large_m) + aes(x=factor(haul), y=log(cpue_wt+1), color=bed_name) + geom_point()
ggplot(large_m) + aes(x=factor(haul), y=log(cpue_wt+1), color=bed_name, shape=factor(gear_code)) + geom_point(size=2) +scale_color_brewer(type="qual", palette=3)+
    theme_cowplot()
#hmm. looks like in YAK 4 the new gear definitely fishes better (I think 5 is the new gear)



ggplot(small_m) + aes(x=factor(haul), y=log(cpue_wt+1)) + geom_boxplot()
ggplot(small_m) + aes(x=factor(haul), y=log(cpue_wt+1), color=bed_name, shape=factor(gear_code)) + geom_point(size=2) + scale_color_brewer(type="qual", palette=3)+
    theme_cowplot()


#temp?
ggplot(large_m) + aes(x=factor(haul), y=log(cpue_wt+1), color=bed_name, shape=factor(gear_code)) + geom_point(size=2) +scale_color_brewer(type="qual", palette=3)+
    theme_cowplot()

ggplot(large_m) + aes(x=factor(haul), y=cpue_wt, color=bed_name, shape=factor(gear_code)) + geom_point(size=2) +scale_color_brewer(type="qual", palette=3)+
    theme_cowplot()

ggplot(large_m) + aes(x=factor(haul), y=cpue_wt, color=gear_code, shape=factor(gear_code)) + geom_point(size=2) +scale_color_brewer(type="seq", palette=5)+
    theme_cowplot()

ggplot(small_m) + aes(x=factor(haul), y=cpue_wt, color=gear_code, shape=factor(gear_code)) + geom_point(size=2) +scale_color_brewer(type="seq", palette=5)+
    theme_cowplot()

```

Residual plots - some possible outliers but no strong patterns. Normality fails.
```{r}
##plot resids
plot(resid(mod1_large_cnt)) #couple of outliers
plot(resid(mod1_large_wt))
plot(resid(mod1_small_cnt)) #couple of outliers
plot(resid(mod1_small_wt))  #up pattern


qqnorm(log(large_m$cpue_wt+1))
qqnorm(log(small_m$cpue_wt + 1))

#formal normality test
shapiro.test(log(large_m$cpue_wt+1))
shapiro.test(log(small_m$cpue_wt+1))
##both fail the formal normality test...
#hist(log(large_m$cpue_wt+1))
hist(log(large$cpue_wt+1))



#plot outliers
plot(mod1_large_wt) #21 and 22 appear to be outliers, This is haul #29. Kodiak caught 2, homer caught 0.
plot(mod1_small_wt) #24, 35, 36, and 23 are outliers. These are hauls 41(indices 23 and 24; kodiak caught 0) and 71(indices 35 and 36; homer caught 0)
plot(mod1_large_cnt)
plot(mod1_small_cnt)

#what if I remove the outliers?
no_outliers_l <- large_m[-c(21,22),]
no_outliers_s <- small_m[-c(23,24,35,36),]

shapiro.test(log(no_outliers_l$cpue_wt+1))
#fails normality test
shapiro.test(log(no_outliers_s$cpue_wt+1))
#passes normaliy test, barely

#do results change if I remove the outliers?
mod1_large_wt_nooutliers <- lm(log(cpue_wt+1) ~ gear_code + haul, data=no_outliers_l)
sum_o<-summary(mod1_large_wt_nooutliers)

s_large_wt_o <- sum_o$coefficients[2,2]  #SE for large scallops
g_large_wt_o <- sum_o$coefficients[2,1] #gear difference parameter for large scallops

FPC_large_wt_o <- exp(2*g_large_wt_o*(1+(0.5*(s_large_wt_o^2)))) 

upper_CI_large_wt_no_outliers <- exp(2*g_large_wt_o + 1.96*2*s_large_wt_o)
lower_CI_large_wt_no_outliers <- exp(2*g_large_wt_o - 1.96*2*s_large_wt_o)

FPC_large_wt_o
upper_CI_large_wt_no_outliers
lower_CI_large_wt_no_outliers #same results, apply FPC. More precision though

#small no outliers
mod1_small_wt_nooutliers  <- lm(log(cpue_wt+1) ~ gear_code + haul, data=no_outliers_s)
sum_o_s<-summary(mod1_small_wt_nooutliers)

s_small_wt_o <- sum_o_s$coefficients[2,2]  #SE for large scallops
g_small_wt_o <- sum_o_s$coefficients[2,1] #gear difference parameter for large scallops

FPC_small_wt_o <- exp(2*g_small_wt_o*(1+(0.5*(s_small_wt_o^2)))) 

upper_CI_small_wt_no_outliers <- exp(2*g_small_wt_o + 1.96*2*s_small_wt_o)
lower_CI_small_wt_no_outliers <- exp(2*g_small_wt_o - 1.96*2*s_small_wt_o)

scallop_FPC_table_2 <- rbind(scallop_FPC_table, c("large no outliers (weight)", FPC_large_wt_o, upper_CI_large_wt_no_outliers, lower_CI_large_wt_no_outliers))

scallop_FPC_table_3 <- rbind(scallop_FPC_table_2, c("small no outliers (weight)", FPC_small_wt_o, upper_CI_small_wt_no_outliers, lower_CI_small_wt_no_outliers))

scallop_FPC_table_3
```
Large scallops: normality fails
Large scallops with outliers removed: normality fails

Small scalops: normality fails
Small scallops with outliers removed: normality barely passes



########################################
Kappenman exploration. NOTE: after fpc(method=2) is run, lm() does not work the same... -> This is because the options(controls) setting is changed in fpc(method=2)
```{r}
#large

#fpc randomized block anova 
dat<- large_m %>% arrange(haul, gear_code)

 tmp1 <- dat %>% filter(gear_code==1)
 a <- tmp1$cpue_wt
  
 tmp2 <- dat %>% filter(gear_code==5)
 b <- tmp2$cpue_wt
#kapp_wt_L1 <- fpc(cpue2, cpue1, method = 4, kapp_zeros = "ind", boot_type = "unpaired") #kapp_zeros="paired"??
#kapp_wt_L2 <- fpc(cpue1, cpue2, method = 4, kapp_zeros = "ind", boot_type = "unpaired")
ANOVA_fpc_l_wt_cheating <- fpc(a, b, method = 2, kapp_zeros = "ind", boot_type = "unpaired")
ANOVA_fpc_l_wt_cheating#same estimate as lm() but wider CI

#small
#dat <- small_m

 # tmp1 = dat %>% filter(gear_code==1)
 # cpue1 <- tmp1$cpue_wt
  
 # tmp2 = dat %>% filter(gear_code==5)
 # cpue2 <- tmp2$cpue_wt


#kapp_wt_S1 <- fpc(cpue2, cpue1, method = 4, kapp_zeros = "ind", boot_type = "unpaired") #kapp_zeros="paired"??
#kapp_wt_S2 <- fpc(cpue1, cpue2, method = 4, kapp_zeros = "ind", boot_type = "unpaired")


#kapp_wt_L1
#kapp_wt_L2 #kodak -> homer
#kapp_wt_S1
#kapp_wt_S2 #kodiak -> homer

```



Kappenman's function - adjusted from crab github code
```{r}
f_kapp_AR <- function(data) {
    #deleted args from function: ves1, ves2, vessel_name = "vessel"
  
  # ensure vessel and cpue columns are properly named
  #data %>%
   # rename(vessel = as.name(vessel_name),
    #       cpue = as.name(cpue_name)) -> data
  
  # pivot data to wide format (i.e., in pairs)
 # data %>%
    #pivot_wider(names_from = vessel, values_from = cpue_wt) -> data
  # extract cpue vectors from data
 # cpue1 = data %>% pull(as.name(ves1)) # standard vessel
 # cpue2 = data %>% pull(as.name(ves2)) 
  
  #alex code
  tmp1 = data %>% filter(gear_code==1)
  cpue1 <- tmp1$cpue_wt
  
  tmp2 = data %>% filter(gear_code==5)
  cpue2 <- tmp2$cpue_wt
  
  # call fishmethods::fpc
  kapp = try(fishmethods::fpc(cpue1, cpue2, method = 4, kapp_zeros = "ind", boot_type = "unpaired"), silent = T)
  #kapp = try(fishmethods::fpc(cpue1, cpue2, method = 4), silent = T)
  #AR adjusted so that cpue 2 is the first number and cpue 1 is the second number
  #cpue 2 is for gear type 5 (new dredge)
  #cpue 1 is for gear type 1(old dredge)
  
  # organize results
  if(class(kapp) == "try-error"){
    tibble(n = NA,
           cpue1 = NA,
           cpue2 = NA,
           fpc = NA,
           boot_se = NA,
           boot_l95 = NA,
           boot_u95 = NA) -> out
  } else{
    kapp %>%
      as_tibble() %>%
      rename(n = n1,
             cpue1 = `mean cpue1`,
             cpue2 = `mean cpue2`,
             fpc = FPC,
             boot_se = Boot_SE,
             boot_l95 = `Boot_95%_LCI`,
             boot_u95 = `Boot_95%_UCI`) %>%
      dplyr::select(n, cpue1, cpue2, fpc, boot_se, boot_l95, boot_u95) -> out
  }
    
  # fix names
  #names(out)[2] = paste0(ves1, "_cpue")
  #names(out)[3] = paste0(ves2, "_cpue")
  
  # output
  return(out)
  
}

#ok ok, that should be a functional function. f_kapp_AR

kapp_wt_L2 <- f_kapp_AR(data=large_m)
kapp_wt_S2 <- f_kapp_AR(data=small_m)
```

MSE sim Kapp - adjsuted from crab github code
```{r}
#AR adjustments
f_kapp_mse_sim_AR1 <- function(data, n, method){
  
  # pull cpue from data
  data %>%
    pull(cpue_wt) -> cpue  
    
  # compute parameters of lognormal dist
  mu = log(mean(cpue)) - 0.5*log(1+(sd(cpue)/mean(cpue))^2)
  var = log(1+(sd(cpue)/mean(cpue))^2) #ah. for the bootstrap. AR.
  
  # check pdf
  #hist(cpue, freq = F)
  #lines(density(rlnorm(1000, meanlog = mu, sdlog = sqrt(var))), col = 2)
  
  cpue_u = NULL
  cpue_c = NULL
  mse_u = NULL
  mse_c = NULL
  fpd = seq(0.1, 2, 0.1) 
  for(i in 1:length(fpd)){
    for(j in 1:200){
      # simulate cpue standard vessel aka the Homer dredge
      hom_sim = rlnorm(n, meanlog = mu, sdlog = sqrt(var)) #simulations for HOMER
      # simulate cpue non-standard vessel aka the Kodiak dredge
      kod_sim = rlnorm(n, meanlog = mu, sdlog = sqrt(var)) * fpd[i] #simulations for KODIAK
      # estimate FPC
      fpc <- fpc(cpue1 = hom_sim, cpue2 = kod_sim, method = method, nboot = 0, kapp_zeros = "ind") #ah. here we can choose if we want method 2 (anova) or 4(kappenmans)
      
      # join vessel cpues to one vector
      ## uncorrected
      survey_u = c(hom_sim, kod_sim)
      ## corrected
      survey_c = c(hom_sim, (kod_sim * fpc$FPC))  # scale to standard vessel
      
      # compute means of uncorrected and corrected data
      cpue_u[j] = mean(survey_u)
      cpue_c[j] = mean(survey_c)
      
    }
    mse_u[i] = mean((cpue_u - mean(cpue))^2)
    mse_c[i] = mean((cpue_c - mean(cpue))^2)
  }
  
  tibble(fpd = fpd,
         mse_u = mse_u,
         mse_c = mse_c)
  
}


### simulation plot function AR
f_sim_plot <- function(data, fpc){
  # plot data
  data %>%
    pivot_longer(cols=c("mse_u", "mse_c"), names_to = "sim", values_to = "mse") %>%
    mutate(sim = case_when(sim == "mse_u" ~ "Uncorrected",
                           sim == "mse_c" ~ "Corrected")) %>%
    ggplot()+
    geom_point(aes(x = fpd, y = mse, color = sim), alpha = 0.3)+
    geom_smooth(aes(x = fpd, y = mse, color = sim), method = "lm",formula = y ~ poly(x, 2), se = F)+
   # scale_color_manual(values = cb_palette[c(1,3)])+
    geom_vline(aes(xintercept = fpc), linetype = 2)+ #add in later? PROBS. AR
    labs(x = "Fishing Power Difference", y = "MSE", color = NULL)+
    scale_x_continuous(breaks = seq(0, 2, 0.2), limits = c(0, 2)) -> x
  # save
 # ggsave(paste0(path_prefix, spp_name, ".png"), plot = x, width = 5, height = 4, units = "in")
  return(x)
}

#test
#test_large <- f_kapp_mse_sim_AR1(data=large_m, n=10, method=4) #what should n be? 48 is the number of hauls (sample size 96)
#f_sim_plot(data=test_large, fpc=FPC_large_wt) #need to chage the fpc to a kapp value, this is the anova output. Kapp value is about 0.84

#kapp
kapp_mse_large <- f_kapp_mse_sim_AR1(data=large_m, n=48, method=4)
f_sim_plot(data=kapp_mse_large, fpc=kapp_wt_L2$fpc) #was FPC
##says dont use FPC(because it increases error) 

kapp_mse_small <- f_kapp_mse_sim_AR1(data=small_m, n=48, method=4)
f_sim_plot(data=kapp_mse_small, fpc=kapp_wt_S2$fpc) #was FPC
#borderline

```

MSE SIM ANOVA using the fpc (which apppears to have the same results as the lm output, when the contrasts are manually set)
```{r}
ANOVA_mse_large <- f_kapp_mse_sim_AR1(data=large_m, n=48, method=2)
f_sim_plot(data=ANOVA_mse_large, fpc=FPC_large_wt) #was FPC
##says dont use FPC(because it increases error) 

ANOVA_mse_small <- f_kapp_mse_sim_AR1(data=small_m, n=48, method=2)
f_sim_plot(data=ANOVA_mse_small, fpc=FPC_small_wt)
```


ANOVA function - adjusted FROM CRAB GITHUB CODE (and unused right now)
```{r}
f_rand_block <- function(data, vessel_name = "gear_code", cpue_name = "cpue_wt") { #why are there quotes in line 1?
  
  # ensure vessel and cpue columns are properly named #why do I need this here?
  #data %>%
   # rename(gear_code = as.name(gear_code),
    #       cpue_wt = as.name(cpue_name)) -> data #not sure why i need this bloc
  
  # use sum contrasts for linear model  #what is this? Not a clue what this is dping. Do I need it?
  options(contrasts = rep("contr.sum", 2)) #this sets a differnt default in R. Do I want this?
    ##this is what messes up the lm function.... dO I want this or the default lm? Is this right? What's going on when I do this and why do I get different results
  
  #mutate(data, pair = as.character(pair)) -> data I think my haul specification works ok
  
  # fit linear model and extract coefficients
  fit <- lm(log(cpue_wt + 1) ~ haul + gear_code, data = data)
  coefs <- summary(fit)[["coefficients"]]
  # number of hauls
  n = length(unique(data$haul)) 
  # grand mean; aka intercept
  mu = coefs[1, 1] 
  # resolution treatment effect
  nu = coefs[nrow(coefs), 1] 
  nu_se = coefs[nrow(coefs), 2] 
  p_val = coefs[nrow(coefs), 4] 
  # fpc
  fpc = exp(2 * nu * (1 + (0.5 * nu_se^2))) 
  l95 = exp((2 * nu) - (1.96 * 2 * nu_se))
  u95 = exp((2 * nu) + (1.96 * 2 * nu_se))
  
  # output a tibble
  tibble(lmod = list(fit),
         n = n,
         mu = mu,
         gear_effect = nu,
         se = nu_se,
         p_val = p_val,
         fpc = fpc,
         l95 = l95,
         u95 = u95)
}
```




# Size comp exploration
- see liens 398 of sbs_fpc_estimateion.R code

Graph
```{r}
#ggplot(specimen) + aes(x=shell_height, y=count) +geom_point() + facet_wrap(~gear_code_description)
#hist(specimen$shell_height)
library(viridis)

ggplot(specimen) + aes(x=shell_height, color=factor(gear_code), group = factor(gear_code)) +geom_density()+
    theme_minimal() #


ggplot(specimen) + aes(x=shell_height, color=factor(gear_code), group = factor(gear_code)) +geom_boxplot()+
    theme_minimal() 

ggplot(specimen) + aes(y=shell_height, x=haul, color=factor(gear_code), group = factor(gear_code), size=sample_factor) +geom_point(alpha=0.7, shape=1)+
    theme_minimal() + scale_color_brewer(type = "qual")

ggplot(specimen) + aes(y=shell_height, x=haul, color=sample_factor) +geom_point(alpha=0.7, shape=1, size=5)+
    theme_minimal() + scale_color_viridis_c() + facet_wrap(~gear_code)

ggplot(specimen) + aes(y=shell_height, x=haul, color=sample_factor, size=sample_factor) +geom_point(alpha=0.5, shape=1)+
    theme_minimal() + scale_color_viridis_c() + facet_wrap(~gear_code)

hist(specimen$sample_factor) #so lots of low numbers for the sample factor. Many hauls had few scallops.
```

More size comp graphs
```{r}
library(viridis)

ggplot(specimen)+ aes(x=shell_height, y=sample_factor, color=as.factor(gear_code)) + #this plot is good
    geom_point(size=3, alpha=0.3)+
    scale_color_viridis_d()+
    theme_minimal()



    
```


Formal test between distributions (unweighted). This will be deleted, as we'll want to use the sample_factor for the weight
```{r}
library(stats)
homer_spec <- specimen %>% filter(gear_code==1)
kodiak_spec <- specimen %>% filter(gear_code==5)
ks.test(x=homer_spec$shell_height, y=kodiak_spec$shell_height, alternative="two.sided")
##This test shows us that we cant reject the null hypothesis: that the dredges have the same scallop height distributions

#If y is numeric, a two-sample (Smirnov) test of the null hypothesis that x and y were drawn from the same distribution is performed. 


##CRAP! I need to weight the shell_heights based on the sample_factor. 
```

Weighted ks test
```{r}
library(Ecume)

ks_weighted_result_4 <- Ecume::ks_test(x=homer_spec$shell_height, y=kodiak_spec$shell_height, thresh= 0, w_x=(homer_spec$sample_factor/100), w_y=(kodiak_spec$sample_factor/100)) #testing H0 D=0?

ks_weighted_result_4 #says do not reject H0, p=.99
```


How to graph something like this? The distributions with weights?
```{r}
ggplot(specimen) + aes(x=shell_height) + geom_density(aes(weight=sample_factor/100))
ggplot(specimen) + aes(x=shell_height, weight= sample_factor/100) + geom_density() #identical
ggplot(specimen) + aes(x=shell_height) + geom_density(aes(weight=sample_factor)) #identical
ggplot(specimen) + aes(x=shell_height) + geom_density(aes(weight=sample_factor)) + facet_wrap(~gear_code, ncol=1) +theme_minimal() #final density plot with weights
ggplot(specimen) + aes(x=shell_height, color=factor(gear_code), group=factor(gear_code)) + geom_density(aes(weight=sample_factor)) +theme_minimal() #final density plot with weights
#unweighted plot below
ggplot(specimen) + aes(x=shell_height, color=factor(gear_code), group=factor(gear_code)) + geom_density() +theme_minimal() #without weights

ggplot(specimen) + aes(x=shell_height) +geom_density() + facet_wrap(~gear_code, ncol=1) + theme_minimal() #density plot without weights

#relative abundance
#ggplot(specimen) + aes(x=shell_height) +geom_histogram() + facet_wrap(~gear_code, ncol=1) + theme_minimal() 
```

Based on the graphs, I think I should do separate weighted KS tests. one for small scallops and one for large scallops.
```{r}
large_spec <- specimen %>% filter(samp_grp == 1)
small_spec <- specimen %>% filter(samp_grp == 2)

ggplot(large_spec) + aes(x=shell_height, color=factor(gear_code), group=factor(gear_code)) + geom_density(aes(weight=sample_factor)) +theme_minimal()

ggplot(small_spec) + aes(x=shell_height, color=factor(gear_code), group=factor(gear_code)) + geom_density(aes(weight=sample_factor)) +theme_minimal()

#can the KS test be used where the distribution is... unconventional? (as it is in the small spec)
homer_spec_l <- large_spec %>% filter(gear_code==1)
kodiak_spec_l <- large_spec %>% filter(gear_code==5)

homer_spec_s <- small_spec %>% filter(gear_code==1)
kodiak_spec_s <- small_spec %>% filter(gear_code==5)

ks_large <-Ecume::ks_test(x=homer_spec_l$shell_height, y=kodiak_spec_l$shell_height, thresh= 0, w_x=(homer_spec_l$sample_factor/100), w_y=(kodiak_spec_l$sample_factor/100)) #should thresh be 0?

ks_small <- Ecume::ks_test(x=homer_spec_s$shell_height, y=kodiak_spec_s$shell_height, thresh= 0, w_x=(homer_spec_s$sample_factor/100), w_y=(kodiak_spec_s$sample_factor/100))

ks_large #do not reject H0, p=.99
ks_small #do not reject H0, p=0.19
```

hIST
```{r}
#density
ggplot(large_spec) + aes(x=shell_height, color=factor(gear_code), group=factor(gear_code)) + geom_density(aes(weight=sample_factor)) +theme_minimal()

ggplot(small_spec) + aes(x=shell_height, color=factor(gear_code), group=factor(gear_code)) + geom_density(aes(weight=sample_factor)) +theme_minimal()

#histogram

ggplot(large_spec) + aes(x=shell_height, color=factor(gear_code), group=factor(gear_code)) + geom_histogram(aes(weight=sample_factor)) +theme_minimal() + facet_wrap(~gear_code, ncol=1)

ggplot(large_spec) + aes(x=shell_height, fill=factor(gear_code), group=factor(gear_code)) + geom_histogram(aes(weight=sample_factor), position="jitter", alpha=0.5) +theme_minimal()


ggplot(small_spec) + aes(x=shell_height, color=factor(gear_code), group=factor(gear_code)) + geom_freqpoly(aes(weight=sample_factor)) +theme_minimal()

ggplot(small_spec) + aes(x=shell_height, color=factor(gear_code), group=factor(gear_code)) + geom_histogram(aes(weight=sample_factor)) +theme_minimal() + facet_wrap(~gear_code, ncol=1)

ggplot(small_spec) + aes(x=shell_height, fill=factor(gear_code), group=factor(gear_code)) + geom_histogram(aes(weight=sample_factor), position="jitter", alpha=0.5) +theme_minimal()

#ggplot(small_spec) + aes(x=shell_height, fill=factor(gear_code), group=factor(gear_code)) + geom_histogram(position="jitter", alpha=0.5) +theme_minimal() #try with no weight
```
Summary table
```{r}
#randomized block ANOVA results (weight and number cpue)
scallop_FPC_table_3 #the one includign outlier results


#kapp results (weight cpue)
kapp_wt_L2 
kapp_wt_S2 #wait mean cpue 1 is larger? Is that right??
```


Gamma glm? - ANOVA
Gamma glmm? - mixed model - haul # as the random variable
```{r}
#glm
##I had to remove 0's in order to the gamma distribution to work.
options(contrasts = rep("contr.sum", 2)) 

glm1_large_wt <- stats::glm(cpue_wt ~ gear_code + haul, data=no_outliers_l, family="Gamma")

glm1_small_wt <- stats::glm(cpue_wt ~ gear_code + haul, data=no_outliers_s, family="Gamma")

AIC(mod1_large_wt_nooutliers, glm1_large_wt) #comparing the no outleir versions because the gamma cannot handle 0's(?)
AIC(mod1_small_wt_nooutliers, glm1_small_wt)
##linear model is better, says AIC.

###can I plug this into the FPC formula the same way as for the gaussian ANOVA?


#glmm - to do later?
#glmmTMB()
#glmer()


```






##########################################################
BELOW IS THE OUTLINE FOR THE DRAFT WRITE UP
SAVE THIS WORK FOR LATER


# Problem statement
A new dredge was needed for the annual weathervane scallop survey, but the original dredge manufacturing details was not documented. It was decided to use a dredge designed and thoroughly tested in the Eastern United States. For the purposes of this project, the old dredge is referred to as the Homer dredge and the new dredge is referenced as the Kodiak dredge. Because the yearly weathervane scallop survey is a time series, there was need for a fishing power comparison between Homer and Kodiak dredges. {Reference the summary provided by Alyssa for revision.}

# Methods

## Study design
To determine if a fishing power correction (FPC) is needed between the Homer dredge and the Kodiak dredge, a paired tow study was conducted. DETAILS. {Reference the summary provided by Alyssa for revision.}

## Statistical analyses

### Fishing power correction (FPC) calculation

The data was separated into large (MEASUREMENT) and small (MEASUREMENT) categories of scallops. This is because of (BIOLOGICAL/FISHERY REASON). CPUE was then calculated by dividing the weight {or number of scallops caught} by the area of the tow:

(CPUE EQUATION)

A randomized block ANOVA {ANCOVA??} was then performed on the data, with indivdual hauls as the randomized blocks (WEIRD WORDING). The CPUE was log-transformed to prevent negative fitted values. The equation is:

{EQUATION} (same as the Spaulinger and Jackson paper)

Where __ represents the cpue, __ is the grand mean {aka intercept}, _g_ is the gear effect of the Homer or Kodiak dredge, __ is the haul effect, and __ represents the error with a normal distribution. This was conducted separately for large and small categories of scallops.

Fishing power correction was calculated as:

{EQUATION; see Spaulinger and Jackson paper page 4}

where (see Spaulinger and Jackson paper) and s^2 is the variance parameter estimate of _g_.

The confidence intervals were calculated as:

{EQUATION; see Spaulinger and Jackson  page 4}

### Mean square error
 {Talk about other studies, the first ones that use MSE (Munro 1998 and other), the ones that use the FPC CI's and also MSE (Spaulinger and Jackson paper and one other) and that you will follow thier example}

### Catch distribution comparison
Sample distributions of the Homer and Kodiak dredges were compared using the Kolmogorov-smirnov test (CITE)


# Results
## Fishing power correction (FPC): to apply or not apply
### Large scallops: weight
### Large scallops: count - DELETE?

TABLE

## Small scallops: weight
## Small scallops: count -DELETE?



# Discussion

# Conclusion

# Next steps for the working draft analysis
Add in the caveats. What survey complications can I quantitatively add into the analysis? Which ones am I unable to add, and thus will remain caveats to the results.

# Draft list of Figures
- map of study area
- Map of hauls (potentially combine with former)
--might take multiple figures
- Dredge pics/diagrams of both Homer and Kodiak dredges
- Catch density comparisons for both L and S scallops
_ FPC graph with CI's? (see Spaulinger and Jackson paper fig 10)
- MSE simulation graphs (at least for L scallops)
- the theoretical FPD (appendix 1 from Spaulinger and Jackson)

# Draft list of tables
- Dredge dimension comparisons
- scallop FPC and CI results


# EXPERIMENTATION CODE BELOW
Graph homer CPUE vs. kodiak CPUE, color coated by haul #
```{r}
names(large_m)


##attempt 3 works!!
#restructured_data_l <- large_m %>%
 # group_by(haul, gear_code) %>%
  #summarise(cpue_wt = sum(cpue_wt)) %>% #does not actually sum, just condenses
  #pivot_wider(names_from = gear_code, values_from = cpue_wt) %>%
  #rename(Haul = haul, Homer_cpue_wt = `1`, Kodiak_cpue_wt = `5`) %>%
  #ungroup()

#attempt 4 - adding reg area
restructured_data_l <- large_m %>%
  group_by(haul, gear_code, reg_area, bed_code) %>%
  summarise(cpue_wt = sum(cpue_wt)) %>%
  pivot_wider(names_from = gear_code, values_from = cpue_wt) %>%
  rename(Haul = haul, Homer_cpue_wt = `1`, Kodiak_cpue_wt = `5`, Reg_Area = reg_area, Bed_Code = bed_code) %>%
  ungroup()


#graph
ggplot(restructured_data_l) + aes(x=Kodiak_cpue_wt, y=Homer_cpue_wt, color=Haul, shape=Reg_Area) + geom_point(size=3) + theme_minimal_grid() +
    scale_color_viridis(discrete=T) #without smoother

ggplot(restructured_data_l) + 
    geom_abline(intercept=0, slope=1, linetype="dashed", color= "red")+
    geom_smooth(aes(x=Kodiak_cpue_wt, y=Homer_cpue_wt)) + geom_point(aes(x=Kodiak_cpue_wt, y=Homer_cpue_wt, color=Haul, shape=Reg_Area),size=3) + theme_minimal_grid() +
    scale_color_viridis(discrete=T) #with smoother (loess)
#linear trend for a while, and then a shift at about 40,000 CPUE to a curve
#is one number FPC (a ratio) appropriate for a Homer -> Kodiak dredge conversion? Would a GAM be better? OR splot at that line

ggplot(restructured_data_l) + 
    geom_abline(intercept=0, slope=1, linetype="dashed", color= "red")+
    geom_smooth(aes(x=Kodiak_cpue_wt, y=Homer_cpue_wt), method="lm") + geom_point(aes(x=Kodiak_cpue_wt, y=Homer_cpue_wt, color=Haul, shape=Reg_Area),size=3) + theme_minimal_grid() +
    scale_color_viridis(discrete=T) #smother with lm

#add in a 1:1 line

ggplot(restructured_data_l) + geom_abline(intercept=0, slope=1, linetype="dashed", color= "red") + geom_point(aes(x=Kodiak_cpue_wt, y=Homer_cpue_wt, color=Haul, shape=Reg_Area),size=3) + theme_minimal_grid() +
    scale_color_viridis(discrete=T)


#now the large
```
Graph conclusions:
As the cpue increases, there is more variation in Kodiak and Homer CPUE

