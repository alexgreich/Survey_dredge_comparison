---
title: "RMD_scallops_8_biologistsummary"
author: "Alex Reich"
date: "2023-10-31"
output: html_document
---

LOOK BACK AT RE-ORIENTATION NOTES... YOu need to make the default KODAK. MAke everything convert to KODIAK. (which is just 1/homer, but do it right)
ERASE ENVIRONMENT AND SEE IF EVERYHTING WORKS

## Take-away message/Abstract NEED TO UPDATE
Apply an FPC for large scallops, but not for small scallops. The size distribution of catch between the Homer and Kodiak dredges are similar, according to the Kolmogorov-Smirnov test. I've identified 2 outliers for the large scallops and 4 outliers for the small scallops. (Hauls where both values are 0 were eliminated from analysis.) Using the CPUE based on weight for analysis, the FPC for a Kodiak to Homer dredge conversion was 1.47 for large scallops and 1.36 for small scallops. Only the large scallops had a confidence interval that did not overlap with one. The plot of predicted values over observed values for the ANOVA has lower accuracy and precision for the higher CPUE hauls.

QC!! ->  results of the MSE simulation indicate that the MSE is reduced when both the large and small scallop FPC's are applied.

Since the large scallop FPC has CI's that do not overlap with one and reduces MSE, the randomized block ANOVA analysis indicates that we should apply the FPC to the large scallops dataset. The small scallop FPC had CI's that did overlap with 1.

I also tried the Kappenman's method. FPC was 1.20 for large scallops and 1.37 for small scallops (using the weight-based CPUE for calculation). Both CI's for the large and small scallop Kappenman FPC's overlap with one, indicating FPC's should not be accepted. This test does not assume normality, which is a plus. However, CPUE's of 0 are deleted and paired hauls are not accounted for, which is undesirable here. 

How important are the paired hauls where one net got 0? Is that a fluke? Should those hauls be removed from analysis? 

In future iterations of analysis, I may want to eliminate the outliers? The outliers occur when one dredge catches 0 scallops and the other one catches not zero scallops.

Results after outliers are removed:
- ANOVA: a higher precision. See table. After removing the outliers, both small and large scallops have confidence intervals outside of 1.




## Analysis

Load packages
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(RColorBrewer)
library(cowplot)
library(fishmethods)
library(viridis)
library(patchwork)

set.seed(123)
```


Read in data. 
```{r}
## logbook data
logbook <- read.csv("./data_folder/new_data_dump/DredgeSurvey_FishLogData_CatchComparisonHauls.csv") 

## catch data
catch_raw <- read.csv("./data_folder/new_data_dump/DredgeSurvey_CatchData_CatchComparisonHauls.csv") 

## specimen data
specimen <- read.csv("./data_folder/new_data_dump/DredgeSurvey_ScallopBioData_CatchComparisonHauls.csv")
```


Data wrangle.
```{r}
# join logbook and catch in a way that preserves zero catches
tidyr::expand_grid(logbook,
            dplyr::distinct(dplyr::transmute(catch_raw, samp_grp, rcode, comname, whole_haul, sample_type))) %>% #expand logbook to all catch types
  dplyr::left_join(catch_raw) %>% #join with the catch.This connects catch vales to gear_code id.
  # fill in zero catches
  tidyr::replace_na(list(samp_cnt = 0, samp_wt = 0)) %>%
  # filter for scallops
  dplyr::filter(rcode == 74120) -> catch # now you have records of scallop catch per size class (samp_grp) per gear type
  

# cpue will be samp_wt (or count) / area_swept_sqnm. This code is calculating CPUE for both weight and number of scallops
catch_cpue <- catch %>% 
    dplyr::mutate(cpue_cnt = samp_cnt/area_swept_sqnm,
           cpue_wt = samp_wt/area_swept_sqnm)

# compute sample factor for specimen data 
specimen %>%
  # group by tow, samp grp, and size to get n at size
  dplyr::group_by(tow, samp_grp, shell_height) %>%
  dplyr::summarize(count = n()) %>%
  # compute number measured per tow
  dplyr::group_by(tow, samp_grp) %>%
  dplyr::mutate(n_measured = n()) %>% dplyr::ungroup() %>%
  # join to catch data
  dplyr::left_join(catch, by = c("tow", "samp_grp")) %>%
  # compute sample factor
  dplyr::mutate(sample_factor = samp_cnt * (count / n_measured)) -> specimen

#separate large and small scallops for cpue
large <- catch_cpue %>% dplyr::filter(samp_grp == 1)
small <- catch_cpue %>% dplyr::filter(samp_grp == 2)

##Remove where both hauls are 0.
large_2 <- large %>% #no change
    dplyr::group_by(haul) %>%
    dplyr::filter(!all(cpue_cnt == 0)) %>%
    dplyr::ungroup()

small_2 <- small %>% #filtered out where both dredges in a haul caught nothing
    dplyr::group_by(haul) %>%
    dplyr::filter(!all(cpue_cnt == 0)) %>%
    dplyr::ungroup()

#code the factors as factors
large_m <- large_2 %>%
           dplyr::mutate(gear_code = base::factor(gear_code, levels=c("5","1")), haul = base::factor(haul))  #the levels argument specifies that Kodiak(5) is the base, to compare Homer(1) against
    

small_m <- small_2 %>%
           dplyr::mutate(gear_code = base::factor(gear_code, levels=c("5", "1")), haul = base::factor(haul)) #levels=c(1,5) ?

```

# Graphs CHOOSE 3
Exploratory plots
```{r}

#logged cpue scale
log_large_plot <- ggplot(large_m) + aes(x=factor(haul), y=log(cpue_wt+1), color=bed_name, shape=factor(gear_code)) + geom_point(size=4) +scale_color_brewer(type="qual", palette=3)+ scale_shape_manual(values=c(19, 15),labels=c("Kodiak dredge", "Homer dredge"))+
    theme_cowplot()+
    xlab("Haul")+ ylab("log(CPUE + 1)")+
    ggtitle("Large scallops")+
    labs(shape="Gear type", color= "Bed name")

log_small_plot <- ggplot(small_m) + aes(x=factor(haul), y=log(cpue_wt+1), color=bed_name, shape=factor(gear_code)) + geom_point(size=4) + scale_color_brewer(type="qual", palette=3)+ scale_shape_manual(values=c(19, 15),labels=c("Kodiak dredge", "Homer dredge"))+
    theme_cowplot() +
    xlab("Haul")+ ylab("log(CPUE + 1)")+
    ggtitle("Small scallops")+
    labs(shape="Gear type", color= "Bed name")

compound_log <- log_large_plot/log_small_plot
compound_log

#ggsave("compound_scallops_log_cpue_wt.png", plot=compound_log, width=20, height=10, units="in")

#regular cpue scale
large_plot <- ggplot(large_m) + aes(x=factor(haul), y=cpue_wt, color=bed_name, shape=factor(gear_code)) + geom_point(size=4) +scale_color_brewer(type="qual", palette=3)+ scale_shape_manual(values=c(19, 15),labels=c("Kodiak dredge", "Homer dredge"))+
    theme_cowplot()+
    xlab("Haul")+ ylab("CPUE (weight)")+
    ggtitle("Large scallops")+
    labs(shape="Gear type", color= "Bed name")

small_plot <- ggplot(small_m) + aes(x=factor(haul), y=cpue_wt, color=bed_name, shape=factor(gear_code)) + geom_point(size=4) + scale_color_brewer(type="qual", palette=3)+ scale_shape_manual(values=c(19, 15),labels=c("Kodiak dredge", "Homer dredge"))+
    theme_cowplot() +
    xlab("Haul")+ ylab("CPUE (weight)")+
    ggtitle("Small scallops")+
    labs(shape="Gear type", color= "Bed name")

compound_reg <- large_plot/small_plot
compound_reg

#ggsave("compound_scallops_regular_cpue_wt.png", plot=compound_reg, width=20, height=10, units="in")

```

Kodiak: Homer plots
```{r}
restructured_data_l <- large_m %>%
  group_by(haul, gear_code, reg_area, bed_code) %>%
  summarise(cpue_wt = sum(cpue_wt)) %>%
  pivot_wider(names_from = gear_code, values_from = cpue_wt) %>%
  rename(Haul = haul, Homer_cpue_wt = `1`, Kodiak_cpue_wt = `5`, Reg_Area = reg_area, Bed_Code = bed_code) %>%
  ungroup()

restructured_data_s <- small_m %>%
  group_by(haul, gear_code, reg_area, bed_code) %>%
  summarise(cpue_wt = sum(cpue_wt)) %>%
  pivot_wider(names_from = gear_code, values_from = cpue_wt) %>%
  rename(Haul = haul, Homer_cpue_wt = `1`, Kodiak_cpue_wt = `5`, Reg_Area = reg_area, Bed_Code = bed_code) %>%
  ungroup()

#plot with loess smoother and red 1:1 line
##large
x<- ggplot(restructured_data_l) + 
    geom_abline(intercept=0, slope=1, linetype="dashed", color= "red")+
    geom_smooth(aes(x=Kodiak_cpue_wt, y=Homer_cpue_wt)) + geom_point(aes(x=Kodiak_cpue_wt, y=Homer_cpue_wt, color=Haul, shape=Reg_Area),size=3) + 
    theme_cowplot() +
    scale_color_viridis(discrete=T)+
    scale_shape_manual(values=c(19, 17, 15), labels=c("D", "E", "K"))+
    xlab("Kodiak CPUE (weight)")+
    ylab("Homer CPUE (weight)")+
    ggtitle("Large scallops")

##small
y<- ggplot(restructured_data_s) + 
    geom_abline(intercept=0, slope=1, linetype="dashed", color= "red")+
    geom_smooth(aes(x=Kodiak_cpue_wt, y=Homer_cpue_wt)) + geom_point(aes(x=Kodiak_cpue_wt, y=Homer_cpue_wt, color=Haul, shape=Reg_Area),size=3) + 
    theme_cowplot() +
    scale_color_viridis(discrete=T)+
    scale_shape_manual(values=c(19, 17, 15), labels=c("D", "E", "K"))+
    xlab("Kodiak CPUE (weight)")+
    ylab("Homer CPUE (weight)")+
    ggtitle("Small scallops")

homer_kodiak <- x+y
homer_kodiak
#ggsave("homer_vs_kodiak.png", plot=homer_kodiak, width=20, height=10, units="in")
    
#that again, but logged  
##large
a<-ggplot(restructured_data_l) + 
    #geom_abline(intercept=0, slope=1, linetype="dashed", color= "red")+
    geom_smooth(aes(x=log(Kodiak_cpue_wt+1), y=log(Homer_cpue_wt+1)), method="lm") + 
    geom_point(aes(x=log(Kodiak_cpue_wt+1), y=log(Homer_cpue_wt+1), color=Haul, shape=Reg_Area),size=3) + 
    theme_cowplot() +
    scale_color_viridis(discrete=T)+
    #scale_shape_manual(values=c(19, 17, 15), labels=c("D", "E", "K"))+
    xlab("Kodiak CPUE (weight)")+
    ylab("Homer CPUE (weight)")+
    ggtitle("Large scallops, logged")+
    labs(shape = "Reg area")#+
    #guides(shape=FALSE, color=FALSE)

##small
b<-ggplot(restructured_data_s) + 
    #geom_abline(intercept=0, slope=1, linetype="dashed", color= "red")+
    geom_smooth(aes(x=log(Kodiak_cpue_wt+1), y=log(Homer_cpue_wt+1)), method="lm") + 
    geom_point(aes(x=log(Kodiak_cpue_wt+1), y=log(Homer_cpue_wt+1), color=Haul, shape=Reg_Area),size=3) + 
    theme_cowplot() +
    scale_color_viridis(discrete=T)+
    #scale_shape_manual(values=c(19, 17, 15), labels=c("D", "E", "K"))+
    xlab("Kodiak CPUE (weight)")+
    ylab("Homer CPUE (weight)")+
    ggtitle("Small scallops, logged")+
    labs(shape="Reg area")

logged_homer_kodiak<-a+b #that worked to combine, without the legends being too werid
logged_homer_kodiak
#save plot
#ggsave("logged_homer_vs_kodiak.png", plot=logged_homer_kodiak, width=20, height=10, units="in")




```


## Data analysis

### ANOVA method
```{r}
options(contrasts = rep("contr.sum", 2)) #to use this or not?????
#options(contrasts = rep("contr.treatment", 2)) #oh. This way does not give me the grand mean as the intercept.
#log(cpue+1) ~ grand mean + net effect (KENAI OR HOMER) + haul effect (HAUL #)
##large scallops
#large_m$gear_code <- relevel(large_m$gear_code, ref="5") #triedd this, decided no

mod1_large_wt <- stats::lm(log(cpue_wt+1) ~ gear_code + haul, data=large_m) #things are different after this runs through. What has happened?
#haul is somewhere? the nammes are named somethign different?
#something about running fishmethods::fpc with method=2 (randomized block anova) messes with the lm function.

mod1_large_cnt <- lm(log(cpue_cnt+1) ~ gear_code + haul, data=large_m)
#mod1_large_cnt_TEST <- aov(log(cpue_cnt+1) ~ grand_mean_cpue_wt + gear_code + haul, data=large_m)
##same same, different way

##small scallops
mod1_small_wt <- lm(log(cpue_wt+1) ~ gear_code + haul, data=small_m)
mod1_small_cnt <- lm(log(cpue_cnt+1) ~ gear_code + haul, data=small_m)

#summary
sum_large_wt <- summary(mod1_large_wt)
sum_large_cnt <- summary(mod1_large_cnt)
sum_small_wt <- summary(mod1_small_wt)
sum_small_cnt <- summary(mod1_small_cnt)

```


Get FPC table from ANOVA
```{r}
##calculate the Fishing Power Correction (FPC) estimator
s_large_wt <- sum_large_wt$coefficients[2,2]  #SE for large scallops
g_large_wt <- sum_large_wt$coefficients[2,1] #gear difference parameter for large scallops

FPC_large_wt <- exp(2*g_large_wt*(1+(0.5*(s_large_wt^2)))) 


s_large_cnt <- sum_large_cnt$coefficients[2,2]  #SE for large scallops
g_large_cnt <- sum_large_cnt$coefficients[2,1] #gear difference parameter for large scallops

FPC_large_cnt <- exp(2*g_large_cnt*(1+0.5*(s_large_cnt^2)))


s_small_wt <- sum_small_wt$coefficients[2,2]  
g_small_wt <- sum_small_wt$coefficients[2,1] 

FPC_small_wt <- exp(2*g_small_wt*(1+0.5*(s_small_wt^2)))


s_small_cnt <- sum_small_cnt$coefficients[2,2]  
g_small_cnt <- sum_small_cnt$coefficients[2,1] 

FPC_small_cnt <- exp(2*g_small_cnt*(1+0.5*(s_small_cnt^2)))

FPC <- c(FPC_large_wt, FPC_large_cnt, FPC_small_wt, FPC_small_cnt)
Category <- c("large (weight)" , "large(count)", "small(weight)", "small(count)")

scallop_FPC_table <- data.frame(Category, FPC)

#CALCULATE CONFIDENCE INTERVALS
##95%CI = exp(2g (+-) 1.96 * 2sqrt(SE^2))

upper_CI_large_wt <- exp(2*g_large_wt + 1.96*2*s_large_wt)
lower_CI_large_wt <- exp(2*g_large_wt - 1.96*2*s_large_wt)
upper_CI_large_cnt<- exp(2*g_large_cnt + 1.96*2*s_large_cnt)
lower_CI_large_cnt<- exp(2*g_large_cnt - 1.96*2*s_large_cnt)
upper_CI_small_wt <- exp(2*g_small_wt + 1.96*2*s_small_wt)
lower_CI_small_wt <- exp(2*g_small_wt - 1.96*2*s_small_wt)
upper_CI_small_cnt <-exp(2*g_small_cnt + 1.96*2*s_small_cnt)
lower_CI_small_cnt <-exp(2*g_small_cnt - 1.96*2*s_small_cnt)


upper_CI <- c(upper_CI_large_wt, upper_CI_large_cnt, upper_CI_small_wt, upper_CI_small_cnt)
lower_CI <- c(lower_CI_large_wt, lower_CI_large_cnt, lower_CI_small_wt,lower_CI_small_cnt)


scallop_FPC_table$Upper_CI <- upper_CI
scallop_FPC_table$Lower_CI <- lower_CI

scallop_FPC_table #apply FPC for large scallops, but not small scallops. Does MSE support this?

#homer *c = kodiak QC'ed to here
```

Says for large apply the FPC, small do not apply the FPC. But I should look at the MSE results as well.


ADD OUTLIER ANALYSIS HERE?? Might beed to go further down?
Residual plots - some possible outliers but no strong patterns. Normality fails.
```{r}
##plot resids
plot(resid(mod1_large_cnt)) #couple of outliers
plot(resid(mod1_large_wt))
plot(resid(mod1_small_cnt)) #couple of outliers
plot(resid(mod1_small_wt))  #up pattern


qqnorm(log(large_m$cpue_wt+1))
qqnorm(log(small_m$cpue_wt + 1))

#formal normality test
shapiro.test(log(large_m$cpue_wt+1))
shapiro.test(log(small_m$cpue_wt+1))
##both fail the formal normality test...
#hist(log(large_m$cpue_wt+1))
hist(log(large$cpue_wt+1))



#plot outliers
plot(mod1_large_wt) #21 and 22 appear to be outliers, This is haul #29. Kodiak caught 2, homer caught 0.
plot(mod1_small_wt) #24, 35, 36, and 23 are outliers. These are hauls 41(indices 23 and 24; kodiak caught 0) and 71(indices 35 and 36; homer caught 0)
plot(mod1_large_cnt)
plot(mod1_small_cnt)

#what if I remove the outliers?
no_outliers_l <- large_m[-c(21,22),]
no_outliers_s <- small_m[-c(23,24,35,36),]

shapiro.test(log(no_outliers_l$cpue_wt+1))
#fails normality test
shapiro.test(log(no_outliers_s$cpue_wt+1))
#passes normaliy test, barely

#do results change if I remove the outliers?
mod1_large_wt_nooutliers <- lm(log(cpue_wt+1) ~ gear_code + haul, data=no_outliers_l)
sum_o<-summary(mod1_large_wt_nooutliers)

s_large_wt_o <- sum_o$coefficients[2,2]  #SE for large scallops
g_large_wt_o <- sum_o$coefficients[2,1] #gear difference parameter for large scallops

FPC_large_wt_o <- exp(2*g_large_wt_o*(1+(0.5*(s_large_wt_o^2)))) 

upper_CI_large_wt_no_outliers <- exp(2*g_large_wt_o + 1.96*2*s_large_wt_o)
lower_CI_large_wt_no_outliers <- exp(2*g_large_wt_o - 1.96*2*s_large_wt_o)

FPC_large_wt_o
upper_CI_large_wt_no_outliers
lower_CI_large_wt_no_outliers #same results, apply FPC. More precision though

#small no outliers
mod1_small_wt_nooutliers  <- lm(log(cpue_wt+1) ~ gear_code + haul, data=no_outliers_s)
sum_o_s<-summary(mod1_small_wt_nooutliers)

s_small_wt_o <- sum_o_s$coefficients[2,2]  #SE for large scallops
g_small_wt_o <- sum_o_s$coefficients[2,1] #gear difference parameter for large scallops

FPC_small_wt_o <- exp(2*g_small_wt_o*(1+(0.5*(s_small_wt_o^2)))) 

upper_CI_small_wt_no_outliers <- exp(2*g_small_wt_o + 1.96*2*s_small_wt_o)
lower_CI_small_wt_no_outliers <- exp(2*g_small_wt_o - 1.96*2*s_small_wt_o)

scallop_FPC_table_2 <- rbind(scallop_FPC_table, c("large no outliers (weight)", FPC_large_wt_o, upper_CI_large_wt_no_outliers, lower_CI_large_wt_no_outliers))

scallop_FPC_table_3 <- rbind(scallop_FPC_table_2, c("small no outliers (weight)", FPC_small_wt_o, upper_CI_small_wt_no_outliers, lower_CI_small_wt_no_outliers))

scallop_FPC_table_3
```


#Kappeman

Kappenman's function - adjusted from crab github code
```{r}
f_kapp_AR <- function(data) {
    #deleted args from function: ves1, ves2, vessel_name = "vessel"
  
  # ensure vessel and cpue columns are properly named
  #data %>%
   # rename(vessel = as.name(vessel_name),
    #       cpue = as.name(cpue_name)) -> data
  
  # pivot data to wide format (i.e., in pairs)
 # data %>%
    #pivot_wider(names_from = vessel, values_from = cpue_wt) -> data
  # extract cpue vectors from data
 # cpue1 = data %>% pull(as.name(ves1)) # standard vessel
 # cpue2 = data %>% pull(as.name(ves2)) 
  
  #alex code
  tmp1 = data %>% filter(gear_code==5)
  cpue1 <- tmp1$cpue_wt
  
  tmp2 = data %>% filter(gear_code==1)
  cpue2 <- tmp2$cpue_wt
  
  # call fishmethods::fpc
  kapp = try(fishmethods::fpc(cpue1, cpue2, method = 4, kapp_zeros = "ind", boot_type = "unpaired"), silent = T)
  #kapp = try(fishmethods::fpc(cpue1, cpue2, method = 4), silent = T)
  #AR adjusted so that cpue 2 is the first number and cpue 1 is the second number
  #cpue 2 is for gear type 5 (new dredge)
  #cpue 1 is for gear type 1(old dredge)
  
  # organize results
  if(class(kapp) == "try-error"){
    tibble(n = NA,
           cpue1 = NA,
           cpue2 = NA,
           fpc = NA,
           boot_se = NA,
           boot_l95 = NA,
           boot_u95 = NA) -> out
  } else{
    kapp %>%
      as_tibble() %>%
      rename(n = n1,
             cpue1 = `mean cpue1`,
             cpue2 = `mean cpue2`,
             fpc = FPC,
             boot_se = Boot_SE,
             boot_l95 = `Boot_95%_LCI`,
             boot_u95 = `Boot_95%_UCI`) %>%
      dplyr::select(n, cpue1, cpue2, fpc, boot_se, boot_l95, boot_u95) -> out
  }
    
  # fix names
  #names(out)[2] = paste0(ves1, "_cpue")
  #names(out)[3] = paste0(ves2, "_cpue")
  
  # output
  return(out)
  
}

#ok ok, that should be a functional function. f_kapp_AR

kapp_wt_L2 <- f_kapp_AR(data=large_m)
kapp_wt_S2 <- f_kapp_AR(data=small_m)
```

MSE sim Kapp - adjsuted from crab github code #DOES THIS WORK?? I just switched from a homer base perspective to a kodiak base perspective. ##Should this fucntion use both homer and kodiak cpue data or just kodiak cpue data?? THAT IS THE QUESTION. 
- ok. and I think I have the answer. We want the sim to look at the total Cpue without correction. So we don't just look at the kodiak baseline then.
```{r}
#AR adjustments
f_kapp_mse_sim_AR1 <- function(data, n, method){ ##Should this fucntion use both homer and kodiak cpue data or just kodiak cpue data?? THAT IS THE QUESTION.
  
  # pull cpue from data
  data %>%
    pull(cpue_wt) -> cpue  #should this be all the cpue or just for the uncorrected? Reference back to Tyler code?
    
  # compute parameters of lognormal dist
  mu = log(mean(cpue)) - 0.5*log(1+(sd(cpue)/mean(cpue))^2) #ok if this is global? OR should these values be just for Kodiak?? We use tge total CPUE to calc the correction?? Should this fucntion use both homer and kodiak cpue data or just kodiak cpue data?? THAT IS THE QUESTION.
  var = log(1+(sd(cpue)/mean(cpue))^2) #ah. for the bootstrap. AR.
  
  # check pdf
  #hist(cpue, freq = F)
  #lines(density(rlnorm(1000, meanlog = mu, sdlog = sqrt(var))), col = 2)
  
  cpue_u = NULL
  cpue_c = NULL
  mse_u = NULL
  mse_c = NULL
  fpd = seq(0.1, 2, 0.1) 
  for(i in 1:length(fpd)){
    for(j in 1:200){
      # simulate cpue standard vessel aka the KODIAK dredge
      kod_sim = rlnorm(n, meanlog = mu, sdlog = sqrt(var)) #simulations for KODIAK(updated) #SHOULD THIS INDICATE ACTUAL mu values for kodiak??
      # simulate cpue non-standard vessel aka the Kodiak dredge
      hom_sim = rlnorm(n, meanlog = mu, sdlog = sqrt(var)) * fpd[i] #simulations for HOMER(updated)
      # estimate FPC
      fpc <- fpc(cpue1 = kod_sim, cpue2 = hom_sim, method = method, nboot = 0, kapp_zeros = "ind") #ah. here we can choose if we want method 2 (anova) or 4(kappenmans). I altered here to make kodiak the default
      
      # join vessel cpues to one vector
      ## uncorrected
      survey_u = c(kod_sim, hom_sim)
      ## corrected
      survey_c = c(kod_sim, (hom_sim * fpc$FPC))  # scale to standard vessel
      
      # compute means of uncorrected and corrected data
      cpue_u[j] = mean(survey_u)
      cpue_c[j] = mean(survey_c)
      
    }
    mse_u[i] = mean((cpue_u - mean(cpue))^2)
    mse_c[i] = mean((cpue_c - mean(cpue))^2)
  }
  
  tibble(fpd = fpd,
         mse_u = mse_u,
         mse_c = mse_c)
  
}


### simulation plot function AR
f_sim_plot <- function(data, fpc){
  # plot data
  data %>%
    pivot_longer(cols=c("mse_u", "mse_c"), names_to = "sim", values_to = "mse") %>%
    mutate(sim = case_when(sim == "mse_u" ~ "Uncorrected",
                           sim == "mse_c" ~ "Corrected")) %>%
    ggplot()+
    geom_point(aes(x = fpd, y = mse, color = sim), alpha = 0.3)+
    geom_smooth(aes(x = fpd, y = mse, color = sim), method = "lm",formula = y ~ poly(x, 2), se = F)+
   # scale_color_manual(values = cb_palette[c(1,3)])+
    geom_vline(aes(xintercept = fpc), linetype = 2)+ #add in later? PROBS. AR
    labs(x = "Fishing Power Difference", y = "MSE", color = NULL)+
    scale_x_continuous(breaks = seq(0, 2, 0.2), limits = c(0, 2)) -> x
  # save
 # ggsave(paste0(path_prefix, spp_name, ".png"), plot = x, width = 5, height = 4, units = "in")
  return(x)
}

#test
#test_large <- f_kapp_mse_sim_AR1(data=large_m, n=10, method=4) #what should n be? 48 is the number of hauls (sample size 96)
#f_sim_plot(data=test_large, fpc=FPC_large_wt) #need to chage the fpc to a kapp value, this is the anova output. Kapp value is about 0.84

#kapp
kapp_mse_large <- f_kapp_mse_sim_AR1(data=large_m, n=48, method=4)
f_sim_plot(data=kapp_mse_large, fpc=kapp_wt_L2$fpc) #was FPC
##says dont use FPC(because it increases error) 

kapp_mse_small <- f_kapp_mse_sim_AR1(data=small_m, n=48, method=4)
f_sim_plot(data=kapp_mse_small, fpc=kapp_wt_S2$fpc) #was FPC
#borderline

```


MSE SIM ANOVA using the fpc (which apppears to have the same results as the lm output, when the contrasts are manually set) - QCing now that Kodiak is the baseline. -CRAP. IT LOOKS THE SAME. MAKE SURE CODE IS USING THE KODIAK MEAN!! AND KODIAK SE!!
```{r}
ANOVA_mse_large <- f_kapp_mse_sim_AR1(data=large_m, n=48, method=2)
f_sim_plot(data=ANOVA_mse_large, fpc=FPC_large_wt) #was FPC
 

ANOVA_mse_small <- f_kapp_mse_sim_AR1(data=small_m, n=48, method=2)
f_sim_plot(data=ANOVA_mse_small, fpc=FPC_small_wt)

##save the plots
png("MSE_large_scallops_ANOVA.png")
f_sim_plot(data=ANOVA_mse_large, fpc=FPC_large_wt) 
dev.off()

png("MSE_small_scallops_ANOVA.png")
f_sim_plot(data=ANOVA_mse_small, fpc=FPC_small_wt) 
dev.off()


```


CONCLUSION TABLES
Summary table
```{r}
#randomized block ANOVA results (weight and number cpue)
scallop_FPC_table_3 #the one includign outlier results #DOES THIS ONE WORK WHEN EVERYTHING RESET??


#kapp results (weight cpue)
kapp_wt_L2 
kapp_wt_S2 #wait mean cpue 1 is larger? Is that right??
```


PREDICTION PLOTS - 11/3/23 adjusted for Kodiak baseline
```{r}

#large scallops fitted values
fitted_values_test <- predict(mod1_large_wt, type="response")
large_m$fitted_cpue_wt <- exp(fitted_values_test)-1

alpha <- ggplot(large_m) + aes(x=haul, shape=gear_code, group= gear_code, size =2) +
    geom_point(aes(y=cpue_wt), color="black", alpha=0.5)+
    geom_point(aes(y=fitted_cpue_wt), color="red", alpha=0.3)+
    theme_cowplot()+
    ggtitle("Predicted values, large scallops")+
    ylab("CPUE (weight)")+
    xlab("Haul")+
    scale_shape_manual(values=c(19, 15),labels=c("Kodiak dredge", "Homer dredge"))+
    labs(shape=NULL)+
    guides(size="none")


#small scallops fitted values

fitted_values_test <- predict(mod1_small_wt, type="response")
small_m$fitted_cpue_wt <- exp(fitted_values_test)-1

beta <- ggplot(small_m) + aes(x=haul, shape=gear_code, group= gear_code, size=2) +
    geom_point(aes(y=cpue_wt), color="black", alpha=0.5)+
    geom_point(aes(y=fitted_cpue_wt), color="red", alpha=0.3)+
    theme_cowplot()+
    ggtitle("Predicted values, small scallops")+
    ylab("CPUE (weight)")+
    xlab("Haul")+
    scale_shape_manual(values=c(19, 15),labels=c("Kodiak dredge", "Homer dredge"))+
    labs(shape=NULL)+
    guides(size="none")

predicted_1 <- alpha/beta
predicted_1

#ggsave("predicted_1.png", plot=predicted_1, width=20, height=10, units="in")


#predicted values: homer-kodiak log graph

#predicted values: homer-kodiak unlogged graph

```
PREDICTED PLOT: Kodaik/homer
```{r}



restructured_data_l_2 <- large_m %>%
  group_by(haul, gear_code, reg_area, bed_code) %>%
  summarise(fitted_cpue_wt = sum(fitted_cpue_wt)) %>%
  pivot_wider(names_from = gear_code, values_from = fitted_cpue_wt) %>%
  rename(Haul = haul, Homer_cpue_wt_pred = `1`, Kodiak_cpue_wt_pred = `5`, Reg_Area = reg_area, Bed_Code = bed_code) %>%
  ungroup()

restructured_data_s_2 <- small_m %>%
  group_by(haul, gear_code, reg_area, bed_code) %>%
  summarise(fitted_cpue_wt = sum(fitted_cpue_wt)) %>%
  pivot_wider(names_from = gear_code, values_from = fitted_cpue_wt) %>%
  rename(Haul = haul, Homer_cpue_wt_pred = `1`, Kodiak_cpue_wt_pred = `5`, Reg_Area = reg_area, Bed_Code = bed_code) %>%
  ungroup()


restructured_data_l$homer_cpue_wt_pred <- restructured_data_l_2$Homer_cpue_wt_pred
restructured_data_l$kodiak_cpue_wt_pred <- restructured_data_l_2$Kodiak_cpue_wt_pred

restructured_data_s$homer_cpue_wt_pred <- restructured_data_s_2$Homer_cpue_wt_pred
restructured_data_s$kodiak_cpue_wt_pred <- restructured_data_s_2$Kodiak_cpue_wt_pred


#obs vs. pred plot: kodiak/homer
xx<- ggplot(restructured_data_l) + 
    #geom_abline(intercept=0, slope=1, linetype="dashed", color= "red")+
    #geom_smooth(aes(x=Kodiak_cpue_wt, y=Homer_cpue_wt)) + 
    geom_point(aes(x=Kodiak_cpue_wt, y=Homer_cpue_wt, color=Haul, shape=Reg_Area),size=3) + 
    geom_point(aes(x=kodiak_cpue_wt_pred, y=homer_cpue_wt_pred), color="red", alpha=0.4) +
    geom_line(aes(x=kodiak_cpue_wt_pred, y=homer_cpue_wt_pred), color="red", alpha=0.4) +
    theme_cowplot() +
    scale_color_viridis(discrete=T)+
    scale_shape_manual(values=c(19, 17, 15), labels=c("D", "E", "K"))+
    xlab("Kodiak CPUE (weight)")+
    ylab("Homer CPUE (weight)")+
    ggtitle("Large scallops with predictions")+
    labs(shape = "Reg area")

##small
yy<- ggplot(restructured_data_s) + 
    #geom_abline(intercept=0, slope=1, linetype="dashed", color= "red")+
    #geom_smooth(aes(x=Kodiak_cpue_wt, y=Homer_cpue_wt)) + 
    geom_point(aes(x=Kodiak_cpue_wt, y=Homer_cpue_wt, color=Haul, shape=Reg_Area),size=3) + 
    geom_point(aes(x=kodiak_cpue_wt_pred, y=homer_cpue_wt_pred), color="red", alpha=0.4) +
    geom_line(aes(x=kodiak_cpue_wt_pred, y=homer_cpue_wt_pred), color="red", alpha=0.4) +
    theme_cowplot() +
    scale_color_viridis(discrete=T)+
    scale_shape_manual(values=c(19, 17, 15), labels=c("D", "E", "K"))+
    xlab("Kodiak CPUE (weight)")+
    ylab("Homer CPUE (weight)")+
    ggtitle("Small scallops with predictions")+
    labs(shape = "Reg area")

zz <- xx+yy
ggsave("homer_vs_kodiak_with_pred.png", plot=zz, width=20, height=10, units="in")

#logged scale
##large
vv<-ggplot(restructured_data_l) + 
    #geom_abline(intercept=0, slope=1, linetype="dashed", color= "red")+
    #geom_smooth(aes(x=log(Kodiak_cpue_wt+1), y=log(Homer_cpue_wt+1)), method="lm") + 
    geom_point(aes(x=log(Kodiak_cpue_wt+1), y=log(Homer_cpue_wt+1), color=Haul, shape=Reg_Area),size=3) + 
    geom_point(aes(x=log(kodiak_cpue_wt_pred+1), y=log(homer_cpue_wt_pred+1)), color="red", alpha=0.4) +
    geom_line(aes(x=log(kodiak_cpue_wt_pred+1), y=log(homer_cpue_wt_pred+1)), color="red", alpha=0.4) +
    theme_cowplot() +
    scale_color_viridis(discrete=T)+
    #scale_shape_manual(values=c(19, 17, 15), labels=c("D", "E", "K"))+
    xlab("Kodiak CPUE (weight)")+
    ylab("Homer CPUE (weight)")+
    ggtitle("Large scallops with predictions, logged")+
    labs(shape = "Reg area")#+
    #guides(shape=FALSE, color=FALSE)

##small
uu<-ggplot(restructured_data_s) + 
    #geom_abline(intercept=0, slope=1, linetype="dashed", color= "red")+
   # geom_smooth(aes(x=log(Kodiak_cpue_wt+1), y=log(Homer_cpue_wt+1)), method="lm") + 
    geom_point(aes(x=log(Kodiak_cpue_wt+1), y=log(Homer_cpue_wt+1), color=Haul, shape=Reg_Area),size=3) + 
    geom_point(aes(x=log(kodiak_cpue_wt_pred+1), y=log(homer_cpue_wt_pred+1)), color="red", alpha=0.4) +
    geom_line(aes(x=log(kodiak_cpue_wt_pred+1), y=log(homer_cpue_wt_pred+1)), color="red", alpha=0.4) +
    theme_cowplot() +
    scale_color_viridis(discrete=T)+
    #scale_shape_manual(values=c(19, 17, 15), labels=c("D", "E", "K"))+
    xlab("Kodiak CPUE (weight)")+
    ylab("Homer CPUE (weight)")+
    ggtitle("Small scallops with predictions, logged")+
    labs(shape="Reg area")

ww<-vv+uu #that worked to combine, without the legends being too werid
ww
#save plot
ggsave("logged_homer_vs_kodiak_withpred.png", plot=ww, width=20, height=10, units="in")


```



DISTRIBUTION PLOTS
```{r}
aa<-hist(specimen$sample_factor) #so lots of low numbers for the sample factor. Many hauls had few scallops.

bb<-ggplot(specimen) + aes(x=shell_height, color=factor(gear_code), group=factor(gear_code)) + geom_density(aes(weight=sample_factor)) +
#facet_wrap(~gear_code, ncol=1) +
theme_cowplot()+ 
scale_color_manual(values = c("blue", "orange"), labels=c("Homer dredge", "Kodiak dredge"))+
xlab("Shell height")+
ylab("Density")+
ggtitle("Shell height density, all scallop sizes")+
labs(color=NULL)+
    theme_cowplot()

#separated into large and small
large_spec <- specimen %>% filter(samp_grp == 1)
small_spec <- specimen %>% filter(samp_grp == 2)

cc<-ggplot(large_spec) + 
    aes(x=shell_height, color=factor(gear_code), group=factor(gear_code)) + 
    geom_density(aes(weight=sample_factor)) +
    theme_cowplot()+
    scale_color_manual(values=c("blue", "orange"), labels=c("Homer dredge", "Kodiak dredge"))+
    xlab("Shell height")+
    ylab("Density")+
    labs(color=NULL)+
    ggtitle("Shell height density, large scallops")

dd<-ggplot(small_spec) +
    aes(x=shell_height, color=factor(gear_code), group=factor(gear_code)) + 
    geom_density(aes(weight=sample_factor)) +
    theme_cowplot()+
    scale_color_manual(values=c("blue", "orange"), labels=c("Homer dredge", "Kodiak dredge"))+
    xlab("Shell height")+
    ylab("Density")+
    labs(color=NULL)+
    ggtitle("Shell height density, small scallops")

    
aa
bb
cc
dd

ccdd <- cc/dd

bb
ccdd

#ggsave("dist_total.png", plot=bb, width=20, height=10, units="in")
#ggsave("dist_sep.png", plot=ccdd, width=15, height=10, units="in")


```

DISTRIBUTION TESTS

Weighted ks test
```{r}
library(Ecume)

homer_spec <- specimen %>% filter(gear_code==1)
kodiak_spec <- specimen %>% filter(gear_code==5)
ks.test(x=homer_spec$shell_height, y=kodiak_spec$shell_height, alternative="two.sided")

ks_weighted_result_4 <- Ecume::ks_test(x=homer_spec$shell_height, y=kodiak_spec$shell_height, thresh= 0, w_x=(homer_spec$sample_factor/100), w_y=(kodiak_spec$sample_factor/100)) #testing H0 D=0?

ks_weighted_result_4 #says do not reject H0, p=.99


#testing for small and large scallops separately, to see what happens
homer_spec_l <- large_spec %>% filter(gear_code==1)
kodiak_spec_l <- large_spec %>% filter(gear_code==5)

homer_spec_s <- small_spec %>% filter(gear_code==1)
kodiak_spec_s <- small_spec %>% filter(gear_code==5)

ks_large <-Ecume::ks_test(x=homer_spec_l$shell_height, y=kodiak_spec_l$shell_height, thresh= 0, w_x=(homer_spec_l$sample_factor/100), w_y=(kodiak_spec_l$sample_factor/100)) #should thresh be 0?

ks_small <- Ecume::ks_test(x=homer_spec_s$shell_height, y=kodiak_spec_s$shell_height, thresh= 0, w_x=(homer_spec_s$sample_factor/100), w_y=(kodiak_spec_s$sample_factor/100))

ks_large #do not reject H0, p=.99
ks_small #do not reject H0, p=0.19
```

